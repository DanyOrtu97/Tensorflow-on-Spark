{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType, ArrayType, FloatType\n",
    "from tensorboard.notebook import display\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import shutil\n",
    "import time\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import uuid\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\" # Must corrispond to the current jdk used by colab\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark/\" # Must corrispond with the downloaded spark (1st line)\n",
    "spark = SparkSession.builder.master(\"spark://192.168.1.38:7077\").appName(\"testTrain\")\\\n",
    "    .config(\"spark.driver.memory\" , \"2g\").\\\n",
    "    config(\"spark.executor.memory\" , \"2g\").\\\n",
    "    enableHiveSupport().getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"Error\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Dropout, Flatten, BatchNormalization\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(320, 320, 1), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(320, 320, 1), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "model = get_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "bc_model_weights = sc.broadcast(model.get_weights())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def fullpath(path, files):\n",
    "    return  [(lambda x: path + x)(x) for x in files]\n",
    "val_dir = \"../chest_xray/chest_xray/val\"\n",
    "filesPneumo = fullpath(val_dir+'/PNEUMONIA/',os.listdir(os.path.join(val_dir, 'PNEUMONIA')))\n",
    "del filesPneumo[0]\n",
    "labelsPneumo = np.zeros(len(filesPneumo))\n",
    "filesNormal = fullpath(val_dir+'/NORMAL/',os.listdir(os.path.join(val_dir, 'NORMAL')))\n",
    "del filesNormal[0]\n",
    "labelsNormal = np.ones(len(filesNormal))\n",
    "fileData = filesPneumo + filesNormal\n",
    "fileLabels = np.concatenate((labelsPneumo,labelsNormal),axis=0).astype(np.float32).tolist()\n",
    "\n",
    "#files."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "file_name = \"image_dataTrain.parquet\"\n",
    "dbfs_file_path = \"../chest_xray/chest_xray/dbfs/\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "'../chest_xray/chest_xray/dbfs/image_dataTrain.parquet'"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_data = []\n",
    "for (file,label) in zip(fileData, fileLabels):\n",
    "  img = Image.open(file)\n",
    "  img = img.resize([320,320])\n",
    "  data = np.asarray( img, dtype=\"float32\" ).reshape([320*320*1])\n",
    "\n",
    "  image_data.append({\"data\": data, \"label\": label})\n",
    "\n",
    "pandas_df = pd.DataFrame(image_data, columns = ['data', 'label'])\n",
    "pandas_df.to_parquet(file_name)\n",
    "#os.makedirs(dbfs_file_path)\n",
    "shutil.copyfile(file_name, dbfs_file_path+file_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "del df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"dbfs/\"+file_name)\n",
    "print(df.count())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1024\")\n",
    "assert len(df.head()) > 0, \"`df` should not be empty\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def parse_image(image_data,image_label):\n",
    "  #image = tf.image.convert_image_dtype(image_data, dtype=tf.float32) * (2. / 255) - 1\n",
    "  image = tf.reshape(image_data,[320,320,1])\n",
    "  return image,image_label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/.local/lib/python3.6/site-packages/pyspark/sql/pandas/functions.py:386: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  \"in the future releases. See SPARK-28264 for more details.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(df.schema, PandasUDFType.SCALAR_ITER)\n",
    "def predict_batch_udf(image_batch_iter):\n",
    "  import tensorflow as tf\n",
    "  config = tf.compat.v1.ConfigProto()\n",
    "  config.gpu_options.allow_growth = True\n",
    "  config.gpu_options.per_process_gpu_memory_fraction=0.6\n",
    "  session = tf.compat.v1.InteractiveSession(config=config)\n",
    "  batch_size = 1\n",
    "  model = get_model()\n",
    "  model.set_weights(bc_model_weights.value)\n",
    "  for image_batch, label_batch in image_batch_iter:\n",
    "    images = np.vstack(image_batch)\n",
    "    labels = label_batch\n",
    "    print(images.dtype)\n",
    "    print(labels.dtype)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images,labels))\n",
    "    dataset = dataset.map(parse_image, num_parallel_calls=12).prefetch(5000).batch(batch_size)\n",
    "    history = model.fit(x=dataset, epochs=5, verbose=True)\n",
    "    yield pd.DataFrame(list(history.history['accuracy']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "@pandas_udf(ArrayType(FloatType()), PandasUDFType.SCALAR_ITER)\n",
    "def predict_batch_udf(image_batch_iter):\n",
    "  def parse_image(image_data):\n",
    "      image = tf.reshape(image_data,[320,320,1])\n",
    "      return image\n",
    "  batch_size = 1\n",
    "  model = get_model()\n",
    "  model.set_weights(bc_model_weights.value)\n",
    "  for image_batch in image_batch_iter:\n",
    "    images = np.vstack(image_batch)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(images)\n",
    "    dataset = dataset.map(parse_image, num_parallel_calls=8).prefetch(5000).batch(batch_size)\n",
    "    preds = model.predict(dataset)\n",
    "    yield pd.Series(list(preds))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def train(datasetDF, epochs=5, batch_size=1, lr=0.001):\n",
    "\n",
    "    import tensorflow as tf\n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.gpu_options.per_process_gpu_memory_fraction=0.6\n",
    "    sess = tf.compat.v1.InteractiveSession(config=config)\n",
    "    model = get_model()\n",
    "    bc_model_weights = sc.broadcast(model.get_weights())\n",
    "    for e in range(epochs):\n",
    "        data, labels = datasetDF\n",
    "        predictions_df = data.select(predict_batch_udf(data).alias(\"prediction\"))\n",
    "        loss = tf.keras.losses.binary_crossentropy(labels, predictions_df.to_numpy())\n",
    "        gradients = tf.keras.backend.gradients(loss, model.trainable_weights)\n",
    "        evaluated_gradients = sess.run(gradients, feed_dict={model.input: np.vstack(data.select(col(\"data\")))})\n",
    "        # For every trainable layer in the network\n",
    "        for i in range(len(model.trainable_weights)):\n",
    "            layer = model.trainable_weights[i]  # Select the layer\n",
    "            sess.run(tf.compat.v1.assign_sub(layer, lr * evaluated_gradients[i]))\n",
    "        bc_model_weights = sc.broadcast(model.get_weights())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-27-149ece3fd19a>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0moutput_file_path\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"../chest_xray/chest_xray/dbfs/history\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mhistory_df\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpredict_batch_udf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"data\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"label\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0malias\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"history\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mhistory_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mshow\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n\u001B[1;32m    438\u001B[0m         \"\"\"\n\u001B[1;32m    439\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtruncate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbool\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mtruncate\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 440\u001B[0;31m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshowString\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m20\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvertical\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    441\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    442\u001B[0m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshowString\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtruncate\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvertical\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1301\u001B[0m             \u001B[0mproto\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mEND_COMMAND_PART\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1303\u001B[0;31m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1304\u001B[0m         return_value = get_return_value(\n\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001B[0m in \u001B[0;36msend_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1031\u001B[0m         \u001B[0mconnection\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_connection\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1032\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1033\u001B[0;31m             \u001B[0mresponse\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconnection\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1034\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mbinary\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1035\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mresponse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_connection_guard\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconnection\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001B[0m in \u001B[0;36msend_command\u001B[0;34m(self, command)\u001B[0m\n\u001B[1;32m   1198\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1199\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1200\u001B[0;31m             \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msmart_decode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstream\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreadline\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1201\u001B[0m             \u001B[0mlogger\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdebug\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Answer received: {0}\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1202\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstartswith\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mproto\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mRETURN_MESSAGE\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3.6/socket.py\u001B[0m in \u001B[0;36mreadinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    584\u001B[0m         \u001B[0;32mwhile\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    585\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 586\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sock\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrecv_into\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    587\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    588\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_timeout_occurred\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "output_file_path = \"../chest_xray/chest_xray/dbfs/history\"\n",
    "history_df = df.select(predict_batch_udf(col(\"data\"),col(\"label\")).alias(\"history\"))\n",
    "history_df.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py:1751: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-28-bb549d86ec82>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-26-8cec608d817a>\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(datasetDF, epochs, batch_size, lr)\u001B[0m\n\u001B[1;32m     10\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0me\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mepochs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m         \u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdatasetDF\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 12\u001B[0;31m         \u001B[0mpredictions_df\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpredict_batch_udf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0malias\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"prediction\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     13\u001B[0m         \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlosses\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbinary_crossentropy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlabels\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpredictions_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_numpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m         \u001B[0mgradients\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackend\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgradients\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrainable_weights\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "train(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-ee7edc48",
   "language": "python",
   "display_name": "PyCharm (BigData)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}