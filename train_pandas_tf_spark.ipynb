{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import initializers\n",
    "#tf.compat.v1.disable_v2_behavior()\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType, ArrayType, FloatType\n",
    "from tensorboard.notebook import display\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import shutil\n",
    "import time\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import uuid\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from pyspark.sql.functions import  col, pandas_udf, PandasUDFType\n",
    "#from systemml.mllearn.estimators import Keras2DML\n",
    "sess = tf.compat.v1.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\" # Must corrispond to the current jdk used by colab\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark/\" # Must corrispond with the downloaded spark (1st line)\n",
    "spark = SparkSession.builder.master(\"spark://192.168.1.38:7077\").appName(\"testTrain\")\\\n",
    "    .config(\"spark.driver.memory\" , \"2g\").\\\n",
    "    config(\"spark.executor.memory\" , \"2g\").\\\n",
    "    enableHiveSupport().getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"Error\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Dropout, Flatten, BatchNormalization\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(320, 320, 1), activation='relu',kernel_initializer=initializers.RandomNormal(stddev=0.01),bias_initializer=initializers.Zeros()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(320, 320, 1), activation='relu',kernel_initializer=initializers.RandomNormal(stddev=0.01),bias_initializer=initializers.Zeros()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu',kernel_initializer=initializers.RandomNormal(stddev=0.01),bias_initializer=initializers.Zeros()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu',kernel_initializer=initializers.RandomNormal(stddev=0.01),bias_initializer=initializers.Zeros()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu',kernel_initializer=initializers.RandomNormal(stddev=0.01),bias_initializer=initializers.Zeros()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu',kernel_initializer=initializers.RandomNormal(stddev=0.01),bias_initializer=initializers.Zeros()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu',kernel_initializer=initializers.RandomNormal(stddev=0.01),bias_initializer=initializers.Zeros()))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid',kernel_initializer=initializers.RandomNormal(stddev=0.01),bias_initializer=initializers.Zeros()))\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "model = get_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "bc_model_weights = sc.broadcast(model.get_weights())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def fullpath(path, files):\n",
    "    return  [(lambda x: path + x)(x) for x in files]\n",
    "val_dir = \"../chest_xray/chest_xray/val\"\n",
    "filesPneumo = fullpath(val_dir+'/PNEUMONIA/',os.listdir(os.path.join(val_dir, 'PNEUMONIA')))\n",
    "del filesPneumo[0]\n",
    "labelsPneumo = np.zeros(len(filesPneumo))\n",
    "filesNormal = fullpath(val_dir+'/NORMAL/',os.listdir(os.path.join(val_dir, 'NORMAL')))\n",
    "del filesNormal[0]\n",
    "labelsNormal = np.ones(len(filesNormal))\n",
    "fileData = filesPneumo + filesNormal\n",
    "fileLabels = np.concatenate((labelsPneumo,labelsNormal),axis=0).astype(np.float32)\n",
    "\n",
    "#files."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "file_name = \"image_dataTrain.parquet\"\n",
    "dbfs_file_path = \"../chest_xray/chest_xray/dbfs/\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "'../chest_xray/chest_xray/dbfs/image_dataTrain.parquet'"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_data = []\n",
    "for (file,label) in zip(fileData, fileLabels):\n",
    "  img = Image.open(file)\n",
    "  img = img.resize([320,320])\n",
    "  data = np.asarray( img, dtype=\"float32\" ).reshape([320*320*1])\n",
    "\n",
    "  image_data.append({\"data\": data, \"label\": label})\n",
    "\n",
    "pandas_df = pd.DataFrame(image_data, columns = ['data', 'label'])\n",
    "pandas_df.to_parquet(file_name)\n",
    "#os.makedirs(dbfs_file_path)\n",
    "shutil.copyfile(file_name, dbfs_file_path+file_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "del df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(file_name)\n",
    "print(df.count())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1024\")\n",
    "assert len(df.head()) > 0, \"`df` should not be empty\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/.local/lib/python3.6/site-packages/pyspark/sql/pandas/functions.py:386: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  \"in the future releases. See SPARK-28264 for more details.\", UserWarning)\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/.local/lib/python3.6/site-packages/pyspark/sql/pandas/functions.py:386: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  \"in the future releases. See SPARK-28264 for more details.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(ArrayType(FloatType()), PandasUDFType.SCALAR_ITER)\n",
    "def predict_test(image_batch_iter):\n",
    "    def parse_image(image_data):\n",
    "        image = tf.reshape(image_data,[320,320,1])\n",
    "        return image\n",
    "    batch_size = 1\n",
    "    model = get_model()\n",
    "    model.set_weights(bc_model_weights.value)\n",
    "    for image_batch in image_batch_iter:\n",
    "        images = np.vstack(image_batch)\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(images)\n",
    "        dataset = dataset.map(parse_image, num_parallel_calls=8).prefetch(5000).batch(batch_size)\n",
    "        preds = model.predict(dataset)\n",
    "        yield pd.Series(list(preds))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/.local/lib/python3.6/site-packages/pyspark/sql/pandas/functions.py:386: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  \"in the future releases. See SPARK-28264 for more details.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(ArrayType(FloatType()), PandasUDFType.SCALAR_ITER)\n",
    "def predict_batch_udf(image_batch_iter):\n",
    "  def parse_image(image_data):\n",
    "      image = tf.reshape(image_data,[320,320,1])\n",
    "      return image\n",
    "  batch_size = 1\n",
    "  model = get_model()\n",
    "  model.set_weights(bc_model_weights.value)\n",
    "  for image_batch in image_batch_iter:\n",
    "    images = np.vstack(image_batch)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(images)\n",
    "    dataset = dataset.map(parse_image, num_parallel_calls=8).prefetch(5000).batch(batch_size)\n",
    "    preds = model.predict(dataset)\n",
    "    yield pd.Series(list(preds))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "def train(datasetDF, epochs=5, batch_size=1, lr=0.001):\n",
    "    def np2Img(arrList):\n",
    "        def parse_image(image_data):\n",
    "            image = image_data.reshape([320,320,1])\n",
    "            return image\n",
    "        conv = []\n",
    "        arrList = arrList.flatten()\n",
    "        for i,el in enumerate(arrList):\n",
    "            conv.append(parse_image(np.asarray(el)))\n",
    "        return np.asarray(conv)\n",
    "    def lists2nparray(arrList, labels=False):\n",
    "        arrList = arrList.flatten()\n",
    "        conv = np.zeros(len(arrList))\n",
    "        for i,el in enumerate(arrList):\n",
    "            if labels:\n",
    "                conv[i] = el\n",
    "            else:\n",
    "                conv[i] = el[0]\n",
    "        return tf.convert_to_tensor(conv.reshape(-1,1))\n",
    "    import tensorflow as tf\n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.gpu_options.per_process_gpu_memory_fraction=0.6\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    sess = tf.compat.v1.InteractiveSession(config=config)\n",
    "    model = get_model()\n",
    "\n",
    "    bce = tf.keras.losses.BinaryCrossentropy()\n",
    "    bc_model_weights = sc.broadcast(model.get_weights())\n",
    "    weights = model.trainable_weights\n",
    "    data = datasetDF[[\"data\"]]\n",
    "    labels = datasetDF[[\"label\"]]\n",
    "    for e in range(epochs):\n",
    "        print(\"Epoch: %d\" %e)\n",
    "        predictions_df = data.select(predict_batch_udf(col(\"data\")).alias(\"prediction\"))\n",
    "\n",
    "        #loss = bce(lists2nparray(labels.toPandas().to_numpy(), labels=True), lists2nparray(predictions_df.toPandas().to_numpy()) )\n",
    "        #print(loss.eval())\n",
    "        gradients = tf.keras.backend.gradients(lists2nparray(predictions_df.toPandas().to_numpy()), model.trainable_weights)\n",
    "        evaluated_gradients = sess.run(gradients, feed_dict={model.input: np2Img(data.select(col(\"data\")).toPandas().to_numpy())})\n",
    "        # For every trainable layer in the network\n",
    "        for i in range(len(model.trainable_weights)):\n",
    "            layer = model.trainable_weights[i]  # Select the layer\n",
    "            sess.run(tf.compat.v1.assign_sub(layer, lr * evaluated_gradients[i]))\n",
    "        bc_model_weights = sc.broadcast(model.get_weights())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Fetch argument None has invalid type <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-78-82658e1029b1>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# pydev_debug_cell\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-77-b5b999c6f42a>\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(datasetDF, epochs, batch_size, lr)\u001B[0m\n\u001B[1;32m     40\u001B[0m             \u001B[0;31m#print(loss.eval())\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     41\u001B[0m             \u001B[0mgradients\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackend\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgradients\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlists2nparray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpredictions_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoPandas\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_numpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrainable_weights\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 42\u001B[0;31m             \u001B[0mevaluated_gradients\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msess\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgradients\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfeed_dict\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m{\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mnp2Img\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"data\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoPandas\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_numpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     43\u001B[0m             \u001B[0;31m# For every trainable layer in the network\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     44\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrainable_weights\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001B[0m in \u001B[0;36mrun\u001B[0;34m(self, fetches, feed_dict, options, run_metadata)\u001B[0m\n\u001B[1;32m    956\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    957\u001B[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001B[0;32m--> 958\u001B[0;31m                          run_metadata_ptr)\n\u001B[0m\u001B[1;32m    959\u001B[0m       \u001B[0;32mif\u001B[0m \u001B[0mrun_metadata\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    960\u001B[0m         \u001B[0mproto_data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf_session\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTF_GetBuffer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrun_metadata_ptr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001B[0m in \u001B[0;36m_run\u001B[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001B[0m\n\u001B[1;32m   1164\u001B[0m     \u001B[0;31m# Create a fetch handler to take care of the structure of fetches.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1165\u001B[0m     fetch_handler = _FetchHandler(\n\u001B[0;32m-> 1166\u001B[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001B[0m\u001B[1;32m   1167\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1168\u001B[0m     \u001B[0;31m# Run request and get response.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, graph, fetches, feeds, feed_handles)\u001B[0m\n\u001B[1;32m    475\u001B[0m     \"\"\"\n\u001B[1;32m    476\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mgraph\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mas_default\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 477\u001B[0;31m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fetch_mapper\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_FetchMapper\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfor_fetch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfetches\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    478\u001B[0m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fetches\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    479\u001B[0m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_targets\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001B[0m in \u001B[0;36mfor_fetch\u001B[0;34m(fetch)\u001B[0m\n\u001B[1;32m    264\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfetch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    265\u001B[0m       \u001B[0;31m# NOTE(touts): This is also the code path for namedtuples.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 266\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0m_ListFetchMapper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfetch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    267\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfetch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcollections_abc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mMapping\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    268\u001B[0m       \u001B[0;32mreturn\u001B[0m \u001B[0m_DictFetchMapper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfetch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, fetches)\u001B[0m\n\u001B[1;32m    376\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    377\u001B[0m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fetch_type\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfetches\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 378\u001B[0;31m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_mappers\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0m_FetchMapper\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfor_fetch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfetch\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mfetch\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mfetches\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    379\u001B[0m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_unique_fetches\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_value_indices\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_uniquify_fetches\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_mappers\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    380\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    376\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    377\u001B[0m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fetch_type\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfetches\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 378\u001B[0;31m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_mappers\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0m_FetchMapper\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfor_fetch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfetch\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mfetch\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mfetches\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    379\u001B[0m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_unique_fetches\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_value_indices\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_uniquify_fetches\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_mappers\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    380\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001B[0m in \u001B[0;36mfor_fetch\u001B[0;34m(fetch)\u001B[0m\n\u001B[1;32m    261\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mfetch\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    262\u001B[0m       raise TypeError('Fetch argument %r has invalid type %r' %\n\u001B[0;32m--> 263\u001B[0;31m                       (fetch, type(fetch)))\n\u001B[0m\u001B[1;32m    264\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfetch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    265\u001B[0m       \u001B[0;31m# NOTE(touts): This is also the code path for namedtuples.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: Fetch argument None has invalid type <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "train(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "d = df[[\"data\"]]\n",
    "#predictions_df = d.select(predict_batch_udf(col(\"data\")).alias(\"prediction\"))\n",
    "x = d.toPandas().to_numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|  prediction|\n",
      "+------------+\n",
      "| [0.5000368]|\n",
      "| [0.5000373]|\n",
      "| [0.5000471]|\n",
      "|   [0.50004]|\n",
      "|   [0.50005]|\n",
      "|[0.50005245]|\n",
      "|[0.50005275]|\n",
      "| [0.5000449]|\n",
      "| [0.5000544]|\n",
      "|[0.50005734]|\n",
      "| [0.5000564]|\n",
      "| [0.5000378]|\n",
      "| [0.5000452]|\n",
      "| [0.5000574]|\n",
      "|[0.50004077]|\n",
      "|[0.50004494]|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_df = df.select(predict_test(col(\"data\")).alias(\"prediction\"))\n",
    "predictions_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "def convert_to_tf(input_df):\n",
    "    x = []\n",
    "    def parse_image(image_data):\n",
    "      image = tf.reshape(image_data,[320,320,1])\n",
    "      return image\n",
    "    input = input_df.toPandas().to_numpy()\n",
    "    for image_batch in input:\n",
    "        for img in image_batch:\n",
    "            x.append(parse_image(img))\n",
    "    return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|label|\n",
      "+-----+\n",
      "|  0.0|\n",
      "|  0.0|\n",
      "|  0.0|\n",
      "|  0.0|\n",
      "|  0.0|\n",
      "|  0.0|\n",
      "|  0.0|\n",
      "|  0.0|\n",
      "|  1.0|\n",
      "|  1.0|\n",
      "|  1.0|\n",
      "|  1.0|\n",
      "|  1.0|\n",
      "|  1.0|\n",
      "|  1.0|\n",
      "|  1.0|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "original_lab = df[[\"label\"]]\n",
    "original_lab.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "bce = tf.keras.losses.BinaryCrossentropy()\n",
    "preds = predictions_df.toPandas().to_numpy()\n",
    "preds = [item for sublist in preds for item in sublist]\n",
    "y_true = original_lab.toPandas().to_numpy()\n",
    "y_pred = preds\n",
    "loss = bce(y_true, y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-20-c90db831fbc8>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0meval\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001B[0m in \u001B[0;36meval\u001B[0;34m(self, feed_dict, session)\u001B[0m\n\u001B[1;32m    911\u001B[0m       \u001B[0mA\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[0marray\u001B[0m \u001B[0mcorresponding\u001B[0m \u001B[0mto\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mvalue\u001B[0m \u001B[0mof\u001B[0m \u001B[0mthis\u001B[0m \u001B[0mtensor\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    912\u001B[0m     \"\"\"\n\u001B[0;32m--> 913\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_eval_using_default_session\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfeed_dict\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgraph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msession\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    914\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    915\u001B[0m   \u001B[0;34m@\u001B[0m\u001B[0mdeprecation\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdeprecated\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"Use ref() instead.\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001B[0m in \u001B[0;36m_eval_using_default_session\u001B[0;34m(tensors, feed_dict, graph, session)\u001B[0m\n\u001B[1;32m   5496\u001B[0m     \u001B[0msession\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_default_session\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   5497\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0msession\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 5498\u001B[0;31m       raise ValueError(\"Cannot evaluate tensor using `eval()`: No default \"\n\u001B[0m\u001B[1;32m   5499\u001B[0m                        \u001B[0;34m\"session is registered. Use `with \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   5500\u001B[0m                        \u001B[0;34m\"sess.as_default()` or pass an explicit session to \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`"
     ]
    }
   ],
   "source": [
    "loss.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "l = original_lab.toPandas().to_numpy()\n",
    "#preds = predictions_df.toPandas().to_numpy()\n",
    "#preds = [item for sublist in preds for item in sublist]\n",
    "#np.asarray(preds).flatten()\n",
    "l"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "[[0.7917439341545105],\n [0.5353220701217651],\n [0.8957020044326782],\n [0.7357274293899536],\n [0.8170340061187744],\n [0.898546576499939],\n [0.8372771739959717],\n [0.7673894166946411],\n [0.896142303943634],\n [0.9164182543754578],\n [0.9064261317253113],\n [0.7997414469718933],\n [0.9225687980651855],\n [0.8096106052398682],\n [0.866844892501831],\n [0.7499334812164307]]"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method NDArrayBase.__del__ of array([0.01102832])>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/mxnet/_ctypes/ndarray.py\", line 58, in __del__\n",
      "    check_call(_LIB.MXNDArrayFree(self.handle))\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <bound method NDArrayBase.__del__ of array([0.01102832])>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/mxnet/_ctypes/ndarray.py\", line 58, in __del__\n",
      "    check_call(_LIB.MXNDArrayFree(self.handle))\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <bound method NDArrayBase.__del__ of array([0.01102832])>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/mxnet/_ctypes/ndarray.py\", line 58, in __del__\n",
      "    check_call(_LIB.MXNDArrayFree(self.handle))\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "from d2l import mxnet as d2l\n",
    "from mxnet import autograd, gluon, init, np, npx\n",
    "\n",
    "with sess.as_default():\n",
    "    def loss_fun(y_true, y_pred):\n",
    "        bce = tf.keras.losses.BinaryCrossentropy()\n",
    "        l = bce(y_true, y_pred)\n",
    "        return tf.reduce_mean(l)\n",
    "    def l():\n",
    "        return loss_fun(y_true, y_pred)\n",
    "    model = get_model()\n",
    "    last_layer = model.layers[-1]\n",
    "    #l = len(model.layers)\n",
    "    trainable = np.array(last_layer.trainable_weights[0].numpy())\n",
    "    trainable.attach_grad()\n",
    "    obj = loss_fun(y_true, y_pred)\n",
    "    train = model.optimizer.minimize(l, tf.convert_to_tensor(trainable))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def get_loss():\n",
    "    return tf.constant(ev)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "layer = model.layers[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "w = layer.trainable_weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "numpy() is only available when eager execution is enabled.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-24-aee7436a6cfa>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mw\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001B[0m in \u001B[0;36mnumpy\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    608\u001B[0m       \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_value\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    609\u001B[0m     raise NotImplementedError(\n\u001B[0;32m--> 610\u001B[0;31m         \"numpy() is only available when eager execution is enabled.\")\n\u001B[0m\u001B[1;32m    611\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    612\u001B[0m   \u001B[0;34m@\u001B[0m\u001B[0mdeprecated\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"Prefer Dataset.range instead.\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNotImplementedError\u001B[0m: numpy() is only available when eager execution is enabled."
     ]
    }
   ],
   "source": [
    "w[0].numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "obj = loss_fun(y_true, y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'conv2d/kernel:0' shape=(3, 3, 1, 32) dtype=float32>\", \"<tf.Variable 'conv2d/bias:0' shape=(32,) dtype=float32>\", \"<tf.Variable 'batch_normalization/gamma:0' shape=(32,) dtype=float32>\", \"<tf.Variable 'batch_normalization/beta:0' shape=(32,) dtype=float32>\", \"<tf.Variable 'conv2d_1_1/kernel:0' shape=(3, 3, 32, 32) dtype=float32>\", \"<tf.Variable 'conv2d_1_1/bias:0' shape=(32,) dtype=float32>\", \"<tf.Variable 'batch_normalization_1/gamma:0' shape=(32,) dtype=float32>\", \"<tf.Variable 'batch_normalization_1/beta:0' shape=(32,) dtype=float32>\", \"<tf.Variable 'conv2d_2/kernel:0' shape=(3, 3, 32, 64) dtype=float32>\", \"<tf.Variable 'conv2d_2/bias:0' shape=(64,) dtype=float32>\", \"<tf.Variable 'batch_normalization_2/gamma:0' shape=(64,) dtype=float32>\", \"<tf.Variable 'batch_normalization_2/beta:0' shape=(64,) dtype=float32>\", \"<tf.Variable 'conv2d_3/kernel:0' shape=(3, 3, 64, 64) dtype=float32>\", \"<tf.Variable 'conv2d_3/bias:0' shape=(64,) dtype=float32>\", \"<tf.Variable 'batch_normalization_3/gamma:0' shape=(64,) dtype=float32>\", \"<tf.Variable 'batch_normalization_3/beta:0' shape=(64,) dtype=float32>\", \"<tf.Variable 'conv2d_4/kernel:0' shape=(3, 3, 64, 128) dtype=float32>\", \"<tf.Variable 'conv2d_4/bias:0' shape=(128,) dtype=float32>\", \"<tf.Variable 'batch_normalization_4/gamma:0' shape=(128,) dtype=float32>\", \"<tf.Variable 'batch_normalization_4/beta:0' shape=(128,) dtype=float32>\", \"<tf.Variable 'conv2d_5/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\", \"<tf.Variable 'conv2d_5/bias:0' shape=(128,) dtype=float32>\", \"<tf.Variable 'batch_normalization_5/gamma:0' shape=(128,) dtype=float32>\", \"<tf.Variable 'batch_normalization_5/beta:0' shape=(128,) dtype=float32>\", \"<tf.Variable 'dense/kernel:0' shape=(165888, 128) dtype=float32>\", \"<tf.Variable 'dense/bias:0' shape=(128,) dtype=float32>\", \"<tf.Variable 'dense_1/kernel:0' shape=(128, 1) dtype=float32>\", \"<tf.Variable 'dense_1/bias:0' shape=(1,) dtype=float32>\"] and loss Tensor(\"Mean:0\", shape=(), dtype=float32).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-35-d37548f440dc>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mopt\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcompat\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mv1\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mAdamOptimizer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0.01\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mtrain\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mopt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mminimize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\u001B[0m in \u001B[0;36mminimize\u001B[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001B[0m\n\u001B[1;32m    408\u001B[0m           \u001B[0;34m\"No gradients provided for any variable, check your graph for ops\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    409\u001B[0m           \u001B[0;34m\" that do not support gradients, between variables %s and loss %s.\"\u001B[0m \u001B[0;34m%\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 410\u001B[0;31m           ([str(v) for _, v in grads_and_vars], loss))\n\u001B[0m\u001B[1;32m    411\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    412\u001B[0m     return self.apply_gradients(grads_and_vars, global_step=global_step,\n",
      "\u001B[0;31mValueError\u001B[0m: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'conv2d/kernel:0' shape=(3, 3, 1, 32) dtype=float32>\", \"<tf.Variable 'conv2d/bias:0' shape=(32,) dtype=float32>\", \"<tf.Variable 'batch_normalization/gamma:0' shape=(32,) dtype=float32>\", \"<tf.Variable 'batch_normalization/beta:0' shape=(32,) dtype=float32>\", \"<tf.Variable 'conv2d_1_1/kernel:0' shape=(3, 3, 32, 32) dtype=float32>\", \"<tf.Variable 'conv2d_1_1/bias:0' shape=(32,) dtype=float32>\", \"<tf.Variable 'batch_normalization_1/gamma:0' shape=(32,) dtype=float32>\", \"<tf.Variable 'batch_normalization_1/beta:0' shape=(32,) dtype=float32>\", \"<tf.Variable 'conv2d_2/kernel:0' shape=(3, 3, 32, 64) dtype=float32>\", \"<tf.Variable 'conv2d_2/bias:0' shape=(64,) dtype=float32>\", \"<tf.Variable 'batch_normalization_2/gamma:0' shape=(64,) dtype=float32>\", \"<tf.Variable 'batch_normalization_2/beta:0' shape=(64,) dtype=float32>\", \"<tf.Variable 'conv2d_3/kernel:0' shape=(3, 3, 64, 64) dtype=float32>\", \"<tf.Variable 'conv2d_3/bias:0' shape=(64,) dtype=float32>\", \"<tf.Variable 'batch_normalization_3/gamma:0' shape=(64,) dtype=float32>\", \"<tf.Variable 'batch_normalization_3/beta:0' shape=(64,) dtype=float32>\", \"<tf.Variable 'conv2d_4/kernel:0' shape=(3, 3, 64, 128) dtype=float32>\", \"<tf.Variable 'conv2d_4/bias:0' shape=(128,) dtype=float32>\", \"<tf.Variable 'batch_normalization_4/gamma:0' shape=(128,) dtype=float32>\", \"<tf.Variable 'batch_normalization_4/beta:0' shape=(128,) dtype=float32>\", \"<tf.Variable 'conv2d_5/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\", \"<tf.Variable 'conv2d_5/bias:0' shape=(128,) dtype=float32>\", \"<tf.Variable 'batch_normalization_5/gamma:0' shape=(128,) dtype=float32>\", \"<tf.Variable 'batch_normalization_5/beta:0' shape=(128,) dtype=float32>\", \"<tf.Variable 'dense/kernel:0' shape=(165888, 128) dtype=float32>\", \"<tf.Variable 'dense/bias:0' shape=(128,) dtype=float32>\", \"<tf.Variable 'dense_1/kernel:0' shape=(128, 1) dtype=float32>\", \"<tf.Variable 'dense_1/bias:0' shape=(1,) dtype=float32>\"] and loss Tensor(\"Mean:0\", shape=(), dtype=float32)."
     ]
    }
   ],
   "source": [
    "opt = tf.compat.v1.train.AdamOptimizer(0.01)\n",
    "train = opt.minimize(obj)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tensorflow.python.framework.ops.EagerTensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-18-8135e6643ba3>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m     \u001B[0mvars\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrainable_weights\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m     \u001B[0mopt\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptimizers\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mAdam\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlearning_rate\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.001\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m     \u001B[0mgradients\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mopt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mminimize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mloss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvar_list\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mvars\u001B[0m \u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m \u001B[0mgradients\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001B[0m in \u001B[0;36mminimize\u001B[0;34m(self, loss, var_list, grad_loss, name)\u001B[0m\n\u001B[1;32m    373\u001B[0m     \"\"\"\n\u001B[1;32m    374\u001B[0m     grads_and_vars = self._compute_gradients(\n\u001B[0;32m--> 375\u001B[0;31m         loss, var_list=var_list, grad_loss=grad_loss)\n\u001B[0m\u001B[1;32m    376\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    377\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply_gradients\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgrads_and_vars\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001B[0m in \u001B[0;36m_compute_gradients\u001B[0;34m(self, loss, var_list, grad_loss)\u001B[0m\n\u001B[1;32m    427\u001B[0m       \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mcallable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvar_list\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    428\u001B[0m         \u001B[0mtape\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwatch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvar_list\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 429\u001B[0;31m       \u001B[0mloss_value\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mloss\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    430\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mcallable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvar_list\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    431\u001B[0m       \u001B[0mvar_list\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvar_list\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: 'tensorflow.python.framework.ops.EagerTensor' object is not callable"
     ]
    }
   ],
   "source": [
    "with sess.as_default():\n",
    "    vars = tf.convert_to_tensor(model.trainable_weights)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    gradients = opt.minimize(loss=loss, var_list=vars )\n",
    "gradients"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape (2, 3) must have rank 0",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-19-7bd5759c3e56>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m     \u001B[0mx1_var\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mVariable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m3\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloat32\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0mx2_var\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mVariable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m4\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m5\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloat32\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m     \u001B[0mcombined_op\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconcat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mx1_var\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx2_var\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m     \u001B[0mgrad_op\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mopt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcompute_gradients\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcombined_op\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    199\u001B[0m     \u001B[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    200\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 201\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mtarget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    202\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mTypeError\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    203\u001B[0m       \u001B[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001B[0m in \u001B[0;36mconcat\u001B[0;34m(values, axis, name)\u001B[0m\n\u001B[1;32m   1650\u001B[0m       ops.convert_to_tensor(\n\u001B[1;32m   1651\u001B[0m           \u001B[0maxis\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"concat_dim\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1652\u001B[0;31m           dtype=dtypes.int32).get_shape().assert_has_rank(0)\n\u001B[0m\u001B[1;32m   1653\u001B[0m       \u001B[0;32mreturn\u001B[0m \u001B[0midentity\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1654\u001B[0m   \u001B[0;32mreturn\u001B[0m \u001B[0mgen_array_ops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconcat_v2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\u001B[0m in \u001B[0;36massert_has_rank\u001B[0;34m(self, rank)\u001B[0m\n\u001B[1;32m   1012\u001B[0m     \"\"\"\n\u001B[1;32m   1013\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrank\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrank\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1014\u001B[0;31m       \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Shape %s must have rank %d\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrank\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1015\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1016\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0mwith_rank\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrank\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: Shape (2, 3) must have rank 0"
     ]
    }
   ],
   "source": [
    "opt = tf.compat.v1.train.AdamOptimizer()\n",
    "with sess.as_default():\n",
    "    x1_var = tf.Variable([1, 2, 3], dtype=tf.float32)\n",
    "    x2_var = tf.Variable([3, 4, 5], dtype=tf.float32)\n",
    "    combined_op = tf.concat(0, [x1_var, x2_var])\n",
    "    grad_op = opt.compute_gradients(combined_op)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_data = convert_to_tf(df[[\"data\"]])\n",
    "x_tensor = tf.convert_to_tensor(input_data, dtype=tf.float32)\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(x_tensor)\n",
    "    output = model(x_tensor)\n",
    "\n",
    "result = output\n",
    "gradients = tf.gradients(output, x_tensor)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "v = gradients[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "X_train = fileData\n",
    "y_train = fileLabels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'InputLayer' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-7-e29ca8539fd6>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0msamples\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m16\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m#max_iter = int(epochs*math.ceil(samples/batch_size))\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0msysml_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mKeras2DML\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mspark\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkeras_model\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput_shape\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m320\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m320\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mweights\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'weights_dir'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[0;34m,\u001B[0m  \u001B[0mtest_interval\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdisplay\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0msysml_model\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/media/andrea/Dati2/BigData/venv/lib/python3.7/site-packages/systemml/mllearn/estimators.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, sparkSession, keras_model, input_shape, transferUsingDF, load_keras_weights, weights, labels, batch_size, max_iter, test_iter, test_interval, display, lr_policy, weight_decay, regularization_type)\u001B[0m\n\u001B[1;32m    949\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkeras_model\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'optimizer'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    950\u001B[0m             \u001B[0mkeras_model\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcompile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'categorical_crossentropy'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mkeras\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptimizers\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSGD\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlr\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.01\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmomentum\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.95\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdecay\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m5e-4\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnesterov\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 951\u001B[0;31m         \u001B[0mkeras2caffe\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconvertKerasToCaffeNetwork\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkeras_model\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m\".proto\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    952\u001B[0m         \u001B[0mkeras2caffe\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconvertKerasToCaffeSolver\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkeras_model\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m\".proto\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m\"_solver.proto\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmax_iter\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtest_iter\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtest_interval\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdisplay\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlr_policy\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mweight_decay\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mregularization_type\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    953\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mweights\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtempfile\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmkdtemp\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mweights\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0mweights\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/media/andrea/Dati2/BigData/venv/lib/python3.7/site-packages/systemml/mllearn/keras2caffe.py\u001B[0m in \u001B[0;36mconvertKerasToCaffeNetwork\u001B[0;34m(kerasModel, outCaffeNetworkFilePath, batch_size)\u001B[0m\n\u001B[1;32m    252\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutCaffeNetworkFilePath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'w'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    253\u001B[0m                 \u001B[0;31m# Write the parsed layers for all but the last layer\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 254\u001B[0;31m                 \u001B[0m_appendKerasLayers\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkerasModel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    255\u001B[0m                 \u001B[0;31m# Now process the last layer with loss\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    256\u001B[0m                 \u001B[0mlastLayer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkerasModel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/media/andrea/Dati2/BigData/venv/lib/python3.7/site-packages/systemml/mllearn/keras2caffe.py\u001B[0m in \u001B[0;36m_appendKerasLayers\u001B[0;34m(fileHandle, kerasLayers, batch_size)\u001B[0m\n\u001B[1;32m    224\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkerasLayers\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    225\u001B[0m                 \u001B[0mtransformedLayers\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mchain\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_iterable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mlayer\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0m_transformLayer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlayer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkerasLayers\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 226\u001B[0;31m                 \u001B[0mjsonLayers\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mchain\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_iterable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mlayer\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0m_parseKerasLayer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlayer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtransformedLayers\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    227\u001B[0m                 \u001B[0mparsedLayers\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mchain\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_iterable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mlayer\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0m_parseJSONObject\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlayer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mjsonLayers\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    228\u001B[0m                 \u001B[0mfileHandle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m''\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparsedLayers\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/media/andrea/Dati2/BigData/venv/lib/python3.7/site-packages/systemml/mllearn/keras2caffe.py\u001B[0m in \u001B[0;36m<lambda>\u001B[0;34m(layer)\u001B[0m\n\u001B[1;32m    224\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkerasLayers\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    225\u001B[0m                 \u001B[0mtransformedLayers\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mchain\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_iterable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mlayer\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0m_transformLayer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlayer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkerasLayers\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 226\u001B[0;31m                 \u001B[0mjsonLayers\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mchain\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_iterable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mlayer\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0m_parseKerasLayer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlayer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtransformedLayers\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    227\u001B[0m                 \u001B[0mparsedLayers\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mchain\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_iterable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mlayer\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0m_parseJSONObject\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlayer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mjsonLayers\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    228\u001B[0m                 \u001B[0mfileHandle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m''\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparsedLayers\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/media/andrea/Dati2/BigData/venv/lib/python3.7/site-packages/systemml/mllearn/keras2caffe.py\u001B[0m in \u001B[0;36m_parseKerasLayer\u001B[0;34m(layer)\u001B[0m\n\u001B[1;32m    143\u001B[0m                 \u001B[0mret\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m \u001B[0;34m'layer'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m{\u001B[0m \u001B[0;34m'name'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mlayer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'type'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m'Data'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mparamName\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mparam\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mparamName\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'top'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mlayer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'top'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m'label'\u001B[0m \u001B[0;34m}\u001B[0m \u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    144\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 145\u001B[0;31m                 \u001B[0mret\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m \u001B[0;34m'layer'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m{\u001B[0m \u001B[0;34m'name'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mlayer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'type'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0msupportedLayers\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mlayerType\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'bottom'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0m_getBottomLayers\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlayer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'top'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mlayer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mparamName\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mparam\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mparamName\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m}\u001B[0m \u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    146\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0;34m[\u001B[0m \u001B[0mret\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_parseActivation\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlayer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlayer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m'_activation'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m]\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0m_shouldParseActivation\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlayer\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;32melse\u001B[0m \u001B[0;34m[\u001B[0m \u001B[0mret\u001B[0m \u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    147\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/media/andrea/Dati2/BigData/venv/lib/python3.7/site-packages/systemml/mllearn/keras2caffe.py\u001B[0m in \u001B[0;36m_getBottomLayers\u001B[0;34m(layer)\u001B[0m\n\u001B[1;32m    115\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    116\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0m_getBottomLayers\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlayer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 117\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0;34m[\u001B[0m \u001B[0mbottomLayer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mbottomLayer\u001B[0m \u001B[0;32min\u001B[0m \u001B[0m_getInboundLayers\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlayer\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    118\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/media/andrea/Dati2/BigData/venv/lib/python3.7/site-packages/systemml/mllearn/keras2caffe.py\u001B[0m in \u001B[0;36m_getInboundLayers\u001B[0;34m(layer)\u001B[0m\n\u001B[1;32m     77\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mnode\u001B[0m \u001B[0;32min\u001B[0m \u001B[0minbound_nodes\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     78\u001B[0m         \u001B[0mnode_list\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnode\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minbound_layers\u001B[0m  \u001B[0;31m# get layers pointing to this node\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 79\u001B[0;31m         \u001B[0min_names\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0min_names\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mextend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnode_list\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     80\u001B[0m     \u001B[0;31m# For Caffe2DML to reroute any use of Flatten layers\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     81\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mchain\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_iterable\u001B[0m\u001B[0;34m(\u001B[0m \u001B[0;34m[\u001B[0m \u001B[0m_getInboundLayers\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ml\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ml\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkeras\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mFlatten\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;34m[\u001B[0m \u001B[0ml\u001B[0m \u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0ml\u001B[0m \u001B[0;32min\u001B[0m \u001B[0min_names\u001B[0m \u001B[0;34m]\u001B[0m \u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: 'InputLayer' object is not iterable"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 1\n",
    "samples = 16\n",
    "#max_iter = int(epochs*math.ceil(samples/batch_size))\n",
    "sysml_model = Keras2DML(spark, keras_model=model, input_shape=(1,320,320), weights='weights_dir', batch_size=batch_size,  test_interval=0, display=10)\n",
    "sysml_model.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-ee7edc48",
   "language": "python",
   "display_name": "PyCharm (BigData)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}