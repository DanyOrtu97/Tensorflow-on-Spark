{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<weakproxy at 0x7f2c746db458 to Device at 0x7f2c8b8fb198>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import mxnet as mx\n",
    "from mxnet.gluon import nn, Trainer, loss\n",
    "from mxnet import optimizer\n",
    "from mxnet import autograd, np, npx, gpu\n",
    "npx.set_np()\n",
    "mx.context.num_gpus()\n",
    "from numba import cuda\n",
    "cuda.select_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = mx.gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpu(0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras import initializers\n",
    "    from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Dropout, Flatten, BatchNormalization\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(320, 320, 1), activation='relu',kernel_initializer=initializers.RandomNormal(stddev=0.01),bias_initializer=initializers.Zeros()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(320, 320, 1), activation='relu',kernel_initializer=initializers.RandomNormal(stddev=0.01),bias_initializer=initializers.Zeros()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu',kernel_initializer=initializers.RandomNormal(stddev=0.01),bias_initializer=initializers.Zeros()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu',kernel_initializer=initializers.RandomNormal(stddev=0.01),bias_initializer=initializers.Zeros()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu',kernel_initializer=initializers.RandomNormal(stddev=0.01),bias_initializer=initializers.Zeros()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu',kernel_initializer=initializers.RandomNormal(stddev=0.01),bias_initializer=initializers.Zeros()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu',kernel_initializer=initializers.RandomNormal(stddev=0.01),bias_initializer=initializers.Zeros()))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid',kernel_initializer=initializers.RandomNormal(stddev=0.01),bias_initializer=initializers.Zeros()))\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "def fullpath(path, files):\n",
    "    return  [(lambda x: path + x)(x) for x in files]\n",
    "val_dir = \"../../chest_xray/chest_xray/val\"\n",
    "filesPneumo = fullpath(val_dir+'/PNEUMONIA/',os.listdir(os.path.join(val_dir, 'PNEUMONIA')))\n",
    "del filesPneumo[0]\n",
    "labelsPneumo = np.zeros(len(filesPneumo))\n",
    "filesNormal = fullpath(val_dir+'/NORMAL/',os.listdir(os.path.join(val_dir, 'NORMAL')))\n",
    "del filesNormal[0]\n",
    "labelsNormal = np.ones(len(filesNormal))\n",
    "fileData = filesPneumo + filesNormal\n",
    "fileLabels = np.concatenate((labelsPneumo,labelsNormal),axis=0).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "model = get_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.backend import clear_session\n",
    "def convert_keras_to_mxnet(model):\n",
    "    def layer_conversion(layer):\n",
    "        t = str(type(layer)).split(\".\")\n",
    "        layer_type = t[-1].replace('\\'>',\"\")\n",
    "        config = layer.get_config()\n",
    "        if layer_type == \"Dense\":\n",
    "            return nn.Dense(units=config['units'], activation=config['activation'], use_bias=config['use_bias'])\n",
    "        if layer_type == \"Conv2D\":\n",
    "            return nn.Conv2D(channels=config['filters'], kernel_size=config['kernel_size'],use_bias=config['use_bias'],activation=config['activation'], weight_initializer=mx.init.Normal(0.01), bias_initializer=mx.init.Zero())\n",
    "        if layer_type == \"MaxPooling2D\":\n",
    "            return nn.MaxPool2D(pool_size=config['pool_size'])\n",
    "        if layer_type == \"BatchNormalization\":\n",
    "            return nn.BatchNorm(momentum=config['momentum'], epsilon=config['epsilon'])\n",
    "        if layer_type==\"Dropout\":\n",
    "            return nn.Dropout(rate=config['rate'])\n",
    "        if layer_type==\"Flatten\":\n",
    "            return nn.Flatten()\n",
    "        return None\n",
    "    mxModel = nn.Sequential()\n",
    "    for layer in model.layers:\n",
    "        new_layer = layer_conversion(layer)\n",
    "        mxModel.add(new_layer)\n",
    "    mxModel.initialize(force_reinit=True, init=mx.init.Xavier())\n",
    "    return mxModel\n",
    "\n",
    "new_model = convert_keras_to_mxnet(model)\n",
    "clear_session()\n",
    "\n",
    "cuda.select_device(0)\n",
    "cuda.close()\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "new_model.initialize(force_reinit=True, init=mx.init.Xavier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<weakproxy at 0x7f2c746db458 to Device at 0x7f2c8b8fb198>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda.select_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "new_model.load_parameters('new_model.params', ctx=gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-420c2f66a370>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'new_model' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Ignoring ../../chest_xray/chest_xray/val/.DS_Store, which is not a directory.\n",
      "  \n",
      "/usr/local/lib/python3.6/dist-packages/mxnet/gluon/data/vision/datasets.py:318: UserWarning: Ignoring ../../chest_xray/chest_xray/val/NORMAL/.DS_Store of type . Only support .jpg, .jpeg, .png\n",
      "  filename, ext, ', '.join(self._exts)))\n",
      "/usr/local/lib/python3.6/dist-packages/mxnet/gluon/data/vision/datasets.py:318: UserWarning: Ignoring ../../chest_xray/chest_xray/val/PNEUMONIA/.DS_Store of type . Only support .jpg, .jpeg, .png\n",
      "  filename, ext, ', '.join(self._exts)))\n"
     ]
    }
   ],
   "source": [
    "from mxnet import image\n",
    "def preprocess(img,label):\n",
    "    img = image.imresize(img/ 255.0,320,320).astype(np.float32)\n",
    "    img = np.swapaxes(img, 0, 2)\n",
    "    img = np.swapaxes(img, 1, 2)\n",
    "    #img = img[np.newaxis, :].astype(np.float32)\n",
    "    return img,label\n",
    "ds2 =  mx.gluon.data.vision.datasets.ImageFolderDataset(\"../../chest_xray/chest_xray/val\",transform=preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_data = mx.gluon.data.DataLoader(ds2, batch_size=16, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_batches(dataset, batch_size, labels):\n",
    "    num_steps = len(dataset)//batch_size\n",
    "    steps = range(num_steps)\n",
    "    data_batches = []\n",
    "    label_batches = []\n",
    "    for step in steps:\n",
    "        batch_idx = range(batch_size) if len(dataset.items[step*batch_size:])>batch_size else range(len(dataset.items[step*batch_size:]))\n",
    "        for idx in batch_idx:\n",
    "            if idx == 0:\n",
    "                img_arr = dataset[step*batch_size+idx][0]\n",
    "\n",
    "            else:\n",
    "                img_arr = np.concatenate((img_arr,dataset[step*batch_size+idx][0]),axis=0)\n",
    "        label_batches.append(labels[step*batch_size:step*batch_size+len(batch_idx)])\n",
    "        data_batches.append(img_arr)\n",
    "    return  data_batches, label_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_batches, label_batches = get_batches(ds2, 16, fileLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_true = np.array([0.,0.,1.,1.]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "params = new_model[-1].collect_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dense_W = params.get(\"weight\").data()\n",
    "dense_B = params.get(\"bias\").data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dense_W.attach_grad()\n",
    "dense_B.attach_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dense_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainer=mx.gluon.Trainer(new_model.collect_params(),'adam', {'learning_rate': 0.001})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "images = ds2[0][0]\n",
    "images = np.concatenate((images,ds2[1][0], ds2[-1][0], ds2[-2][0]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: [100.]; Loss: [0.69314724 0.69314724 0.31326175 0.31326175]\n"
     ]
    }
   ],
   "source": [
    "with autograd.record():\n",
    "    y_ =  new_model.forward(images)\n",
    "    lossValue = BCE(y_, y_true)\n",
    "y_[y_<0.5]=0\n",
    "y_[y_>=0.5]=1\n",
    "acc = (sum(y_==y_true)/len(y_true))*100\n",
    "print( \"acc: \" + str(acc) + \"; Loss: \" + str(lossValue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def acc(output, label):\n",
    "    # output: (batch, num_output) float32 ndarray\n",
    "    # label: (batch, ) int32 ndarray\n",
    "    pred = np.zeros(len(output))\n",
    "    print(\"Predictions: \" + str(output) + \";  Ground Truth: \" + str(label))\n",
    "    for i,o in enumerate(output):\n",
    "        pred[i] = 0. if o < 0.5 else 1.\n",
    "    return (pred ==\n",
    "            label.astype('float32')).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(dataset_batches, labels, epochs, learning_rate):\n",
    "    train_loss, train_acc = 0., 0.\n",
    "    trainerTEST = mx.gluon.Trainer(new_model.collect_params(),'adam', {'learning_rate': learning_rate})\n",
    "    BCE = loss.SigmoidBinaryCrossEntropyLoss()\n",
    "    epochs = range(epochs)\n",
    "    for epoch in epochs:\n",
    "        for data_batch, labels_batch in zip(dataset_batches, labels):\n",
    "            y__true = labels_batch.reshape(-1,1)\n",
    "            with autograd.record():\n",
    "                y__ =  new_model(data_batch)\n",
    "                lossValueTEST = BCE(y__, y__true)\n",
    "            lossValueTEST.backward()\n",
    "            trainerTEST.step(16)\n",
    "        train_loss = lossValueTEST.mean()\n",
    "        train_acc = acc(y__, y__true)\n",
    "        print( \"Epoch: \"+ str(epoch+1) +\"; acc: \" + str(train_acc)+\"; Loss: \" + str(train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1; acc: 0.5; Loss: 0.73598045\n",
      "Epoch: 2; acc: 0.5; Loss: 0.6931472\n",
      "Epoch: 3; acc: 0.5; Loss: 0.6931472\n",
      "Epoch: 4; acc: 0.5; Loss: 0.6931472\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-2c1384873e9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-eabb85d12d2d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset_batches, labels, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossValueTEST\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my__true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"Epoch: \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\"; acc: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"; Loss: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/mxnet/numpy/multiarray.py\u001b[0m in \u001b[0;36m__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    926\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0;34m\"\"\"Returns a string representation of the array.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m         \u001b[0marray_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cpu'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2533\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2534\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2535\u001b[0;31m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[1;32m   2536\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(data_batches,label_batches, 10, 0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bl = label_batches[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bl2 = bl.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "del data_batches, label_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "HL = mx.gluon.loss.SigmoidBinaryCrossEntropyLoss()\n",
    "trainer = mx.gluon.Trainer(new_model.collect_params(),'sgd', {'learning_rate': 0.0005})\n",
    "for epoch in range(30):\n",
    "    train_loss, train_acc, valid_acc = 0., 0., 0.\n",
    "    tic = time.time()\n",
    "    for data, label in train_data:\n",
    "            # forward + backward\n",
    "        with autograd.record():\n",
    "            output = new_model(data)\n",
    "            loss = HL(output.astype(np.float64), label.astype(np.float64))\n",
    "        loss.backward()\n",
    "            # update parameters\n",
    "        trainer.step(16)\n",
    "            # calculate training metrics\n",
    "        train_loss += loss.mean()\n",
    "        train_acc += acc(output, label)\n",
    "        # calculate validation accuracy\n",
    "    print(\"Epoch %d: loss %f, train acc %.3f, in %.1f sec\" % (\n",
    "                epoch, train_loss/len(train_data), train_acc/len(train_data), time.time()-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (ProgettoDLA-Puglisi-)",
   "language": "python",
   "name": "pycharm-43a80b0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
