{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from keras import initializers\n",
    "    from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Dropout, Flatten, BatchNormalization\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(320, 320, 1), activation='relu',kernel_initializer=initializers.RandomNormal(stddev=0.01),bias_initializer=initializers.Zeros()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(320, 320, 1), activation='relu',kernel_initializer=initializers.RandomNormal(stddev=0.01),bias_initializer=initializers.Zeros()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu',kernel_initializer=initializers.RandomNormal(stddev=0.01),bias_initializer=initializers.Zeros()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu',kernel_initializer=initializers.RandomNormal(stddev=0.01),bias_initializer=initializers.Zeros()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu',kernel_initializer=initializers.RandomNormal(stddev=0.01),bias_initializer=initializers.Zeros()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu',kernel_initializer=initializers.RandomNormal(stddev=0.01),bias_initializer=initializers.Zeros()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu',kernel_initializer=initializers.RandomNormal(stddev=0.01),bias_initializer=initializers.Zeros()))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid',kernel_initializer=initializers.RandomNormal(stddev=0.01),bias_initializer=initializers.Zeros()))\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from mxnet import np\n",
    "import os\n",
    "def fullpath(path, files):\n",
    "    return  [(lambda x: path + x)(x) for x in files]\n",
    "val_dir = \"../chest_xray/chest_xray/test\"\n",
    "filesPneumo = fullpath(val_dir+'/PNEUMONIA/',os.listdir(os.path.join(val_dir, 'PNEUMONIA')))\n",
    "del filesPneumo[0]\n",
    "labelsPneumo = np.zeros(len(filesPneumo))\n",
    "filesNormal = fullpath(val_dir+'/NORMAL/',os.listdir(os.path.join(val_dir, 'NORMAL')))\n",
    "del filesNormal[0]\n",
    "labelsNormal = np.ones(len(filesNormal))\n",
    "fileData = filesPneumo + filesNormal\n",
    "fileLabels = np.concatenate((labelsPneumo,labelsNormal),axis=0).astype(np.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = get_model()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "ctx = mx.gpu(0)\n",
    "from mxnet.gluon import nn, Trainer, loss\n",
    "from mxnet import optimizer\n",
    "from mxnet import autograd, np, npx\n",
    "def convert_keras_to_mxnet(model):\n",
    "    npx.set_np()\n",
    "    def layer_conversion(layer):\n",
    "        t = str(type(layer)).split(\".\")\n",
    "        layer_type = t[-1].replace('\\'>',\"\")\n",
    "        config = layer.get_config()\n",
    "        if layer_type == \"Dense\":\n",
    "            return nn.Dense(units=config['units'], activation=config['activation'], use_bias=config['use_bias'], dtype=np.float32)\n",
    "        if layer_type == \"Conv2D\":\n",
    "            return nn.Conv2D(channels=config['filters'], kernel_size=config['kernel_size'],use_bias=config['use_bias'],activation=config['activation'], weight_initializer=mx.init.Normal(0.01), bias_initializer=mx.init.Zero())\n",
    "        if layer_type == \"MaxPooling2D\":\n",
    "            return nn.MaxPool2D(pool_size=config['pool_size'])\n",
    "        if layer_type == \"BatchNormalization\":\n",
    "            return nn.BatchNorm(momentum=config['momentum'], epsilon=config['epsilon'])\n",
    "        if layer_type==\"Dropout\":\n",
    "            return nn.Dropout(rate=config['rate'])\n",
    "        if layer_type==\"Flatten\":\n",
    "            return nn.Flatten()\n",
    "        return None\n",
    "    mxModel = nn.Sequential()\n",
    "    for layer in model.layers:\n",
    "        new_layer = layer_conversion(layer)\n",
    "        mxModel.add(new_layer)\n",
    "    return mxModel\n",
    "\n",
    "new_model = convert_keras_to_mxnet(model)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "new_model.initialize(force_reinit=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/.local/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Ignoring ../chest_xray/chest_xray/train/.DS_Store, which is not a directory.\n",
      "  \n",
      "/usr/local/lib/python3.6/dist-packages/mxnet/gluon/data/vision/datasets.py:318: UserWarning: Ignoring ../chest_xray/chest_xray/train/NORMAL/.DS_Store of type . Only support .jpg, .jpeg, .png\n",
      "  filename, ext, ', '.join(self._exts)))\n",
      "/usr/local/lib/python3.6/dist-packages/mxnet/gluon/data/vision/datasets.py:318: UserWarning: Ignoring ../chest_xray/chest_xray/train/PNEUMONIA/.DS_Store of type . Only support .jpg, .jpeg, .png\n",
      "  filename, ext, ', '.join(self._exts)))\n"
     ]
    }
   ],
   "source": [
    "from mxnet import image\n",
    "def preprocess(img,label):\n",
    "    img = image.imresize(img,320,320)\n",
    "    img = np.swapaxes(img, 0, 2)\n",
    "    img = np.swapaxes(img, 1, 2)\n",
    "    img = img[np.newaxis, :].astype(np.float32)\n",
    "    return img,label\n",
    "ds2 =  mx.gluon.data.vision.datasets.ImageFolderDataset(\"../chest_xray/chest_xray/train\",transform=preprocess)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def get_batches(dataset, batch_size, labels):\n",
    "    num_steps = len(dataset)//batch_size\n",
    "    steps = range(num_steps)\n",
    "    data_batches = []\n",
    "    label_batches = []\n",
    "    for step in steps:\n",
    "        batch_idx = range(batch_size) if len(dataset.items[step*batch_size:])>batch_size else range(len(dataset.items[step*batch_size:]))\n",
    "        for idx in batch_idx:\n",
    "            if idx == 0:\n",
    "                img_arr = dataset[step*batch_size+idx][0]\n",
    "\n",
    "            else:\n",
    "                img_arr = np.concatenate((img_arr,dataset[step*batch_size+idx][0]),axis=0)\n",
    "        label_batches.append(labels[step*batch_size:step*batch_size+len(batch_idx)])\n",
    "        data_batches.append(img_arr)\n",
    "    return  data_batches, label_batches"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "data_batches, label_batches = get_batches(ds2, 8, fileLabels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "y_true = np.array([0.,0.,1.,1.]).reshape(-1,1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "params = new_model[-1].collect_params()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dense_W = params.get(\"weight\").data()\n",
    "dense_B = params.get(\"bias\").data()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dense_W.attach_grad()\n",
    "dense_B.attach_grad()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dense_W"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "trainer=mx.gluon.Trainer(new_model.collect_params(),'adam', {'learning_rate': 0.001})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "images = ds2[0][0]\n",
    "images = np.concatenate((images,ds2[1][0], ds2[-1][0], ds2[-2][0]), axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: [100.]; Loss: [0.69314724 0.69314724 0.31326175 0.31326175]\n"
     ]
    }
   ],
   "source": [
    "with autograd.record():\n",
    "    y_ =  new_model.forward(images)\n",
    "    lossValue = BCE(y_, y_true)\n",
    "y_[y_<0.5]=0\n",
    "y_[y_>=0.5]=1\n",
    "acc = (sum(y_==y_true)/len(y_true))*100\n",
    "print( \"acc: \" + str(acc) + \"; Loss: \" + str(lossValue))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(dataset_batches, labels, epochs, learning_rate):\n",
    "    trainer = mx.gluon.Trainer(new_model.collect_params(),'adam', {'learning_rate': learning_rate})\n",
    "    BCE = loss.SigmoidBinaryCrossEntropyLoss()\n",
    "    epochs = range(epochs)\n",
    "    for epoch in epochs:\n",
    "        for data_batch, labels_batch in zip(dataset_batches, labels):\n",
    "            y_true = labels_batch.reshape(-1,1)\n",
    "            with autograd.record():\n",
    "                y_ =  new_model.forward(data_batch)\n",
    "                lossValue = BCE(y_, y_true)\n",
    "            lossValue.backward()\n",
    "            trainer.step(data_batch.shape[0])\n",
    "        y_[y_<0.5]=0\n",
    "        y_[y_>=0.5]=1\n",
    "        acc = (sum(y_==y_true)/len(y_true))*100\n",
    "        print( \"Epoch: \"+ str(epoch+1) +\"; \"+ \"acc: \" + str(acc) + \"; Loss: \" + str(lossValue))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(data_batches,label_batches, 10, 0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "bl = label_batches[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "bl2 = bl.reshape(-1,1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "del data_batches, label_batches"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-ee7edc48",
   "language": "python",
   "display_name": "PyCharm (BigData)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}