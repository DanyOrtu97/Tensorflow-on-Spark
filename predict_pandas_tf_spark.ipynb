{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType, ArrayType, FloatType\n",
    "from tensorboard.notebook import display\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import shutil\n",
    "import time\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import uuid\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from pyspark.sql.functions import  pandas_udf, PandasUDFType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\" # Must corrispond to the current jdk used by colab\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark/\" # Must corrispond with the downloaded spark (1st line)\n",
    "spark = SparkSession.builder.master(\"spark://192.168.1.38:7077\").appName(\"testTrain\").enableHiveSupport().getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"Error\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Dropout, Flatten, BatchNormalization\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(320, 320, 1), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(320, 320, 1), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "model = get_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "bc_model_weights = sc.broadcast(model.get_weights())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "8"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fullpath(path, files):\n",
    "    return  [(lambda x: path + x)(x) for x in files]\n",
    "val_dir = \"../chest_xray/chest_xray/val\"\n",
    "files = fullpath(val_dir+'/PNEUMONIA/',os.listdir(os.path.join(val_dir, 'PNEUMONIA')))\n",
    "del files[0]\n",
    "len(files)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "'../chest_xray/chest_xray/dbfs/image_data2.parquet'"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = \"image_data2.parquet\"\n",
    "dbfs_file_path = \"../chest_xray/chest_xray/dbfs/\"\n",
    "image_data = []\n",
    "for file in files:\n",
    "  img = Image.open(file)\n",
    "  img = img.resize([320,320])\n",
    "  data = np.asarray( img, dtype=\"float32\" ).reshape([320*320*1])\n",
    "\n",
    "  image_data.append({\"data\": data})\n",
    "\n",
    "pandas_df = pd.DataFrame(image_data, columns = ['data'])\n",
    "pandas_df.to_parquet(file_name)\n",
    "#os.makedirs(dbfs_file_path)\n",
    "shutil.copyfile(file_name, dbfs_file_path+file_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"dbfs/\"+file_name)\n",
    "print(df.count())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1024\")\n",
    "assert len(df.head()) > 0, \"`df` should not be empty\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def parse_image(image_data):\n",
    "  #image = tf.image.convert_image_dtype(image_data, dtype=tf.float32) * (2. / 255) - 1\n",
    "  image = tf.reshape(image_data,[320,320,1])\n",
    "  return image"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/.local/lib/python3.6/site-packages/pyspark/sql/pandas/functions.py:386: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  \"in the future releases. See SPARK-28264 for more details.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(ArrayType(FloatType()), PandasUDFType.SCALAR_ITER)\n",
    "def predict_batch_udf(image_batch_iter):\n",
    "  batch_size = 1\n",
    "  model = get_model()\n",
    "  model.set_weights(bc_model_weights.value)\n",
    "  for image_batch in image_batch_iter:\n",
    "    images = np.vstack(image_batch)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(images)\n",
    "    dataset = dataset.map(parse_image, num_parallel_calls=8).prefetch(5000).batch(batch_size)\n",
    "    preds = model.predict(dataset)\n",
    "    yield pd.Series(list(preds))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "output_file_path = \"../chest_xray/chest_xray/dbfs/predictions\"\n",
    "predictions_df = df.select(predict_batch_udf(col(\"data\")).alias(\"prediction\"))\n",
    "predictions_df.write.mode(\"overwrite\").parquet(output_file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|   prediction|\n",
      "+-------------+\n",
      "| [0.11440846]|\n",
      "| [0.41464415]|\n",
      "| [0.07537913]|\n",
      "|  [0.0463596]|\n",
      "| [0.03866735]|\n",
      "| [0.15374437]|\n",
      "|[0.077885926]|\n",
      "| [0.42321396]|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = spark.read.load(output_file_path)\n",
    "result_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-ee7edc48",
   "language": "python",
   "display_name": "PyCharm (BigData)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}